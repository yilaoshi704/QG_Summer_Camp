# 激活函数

## 1.sigmoid

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
\
$$
在深度学习中，由于其单增以及反函数单增等性质，Sigmoid函数常被用作神经网络的激活函数，将变量映射到[ 0 , 1 ] [0, 1][0,1]之间。

Sigmoid函数的输出范围是0到1。由于输出值限定在0到1，因此它对每个神经元的输出进行了归一化。
用于将预测概率作为输出的模型。由于概率的取值范围是0到1，因此Sigmoid函数非常合适
梯度平滑，避免跳跃的输出值
函数是可微的。这意味着可以找到任意两个点的Sigmoid曲线的斜率
明确的预测，即非常接近1或0。
函数输出不是以0为中心的，这会降低权重更新的效率
Sigmoid函数执行指数运算，计算机运行得较慢。



## 2.Tanh 函数（双曲正切函数）

$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

双曲正切函数是双曲函数的一种。双曲正切函数在数学语言上一般写作tanh 。它解决了Sigmoid函数的不以0为中心输出问题，然而，梯度消失的问题和幂运算的问题仍然存在。



## 3.ReLU 函数（Rectified Linear Unit)

$$
\text{ReLU}(x) = \max(0, x)
$$





线性整流函数（ReLU函数）的特点：

当输入为正时，不存在梯度饱和问题。
计算速度快得多。ReLU 函数中只存在线性关系，因此它的计算速度比Sigmoid函数和tanh函数更快。
Dead ReLU问题。当输入为负时，ReLU完全失效，在正向传播过程中，这不是问题。有些区域很敏感，有些则不敏感。但是在反向传播过程中，如果输入负数，则梯度将完全为零，Sigmoid函数和tanh函数也具有相同的问题
**ReLU函数的输出为0或正数，这意味着ReLU函数不是以0为中心的函数。**

## Leaky ReLU 函数

$$
\text{Leaky ReLU}(x, 0.1x) = \max(0.1x, x) \quad \text{for } x < 0
$$

## Parametric ReLU（PReLU）

$$
\text{PReLU}(x, \alpha) = \max(\alpha x, x)
$$

其中 \(\alpha\) 是一个可学习的参数。

## Exponential Linear Unit（ELU）

$$
\text{ELU}(x, \alpha) = \begin{cases} 
x & \text{if } x > 0 \\
\alpha(e^x - 1) & \text{if } x \leq 0 
\end{cases}
$$



1. **Softmax 函数**
   \[
   \text{Softmax}(\mathbf{x}_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}
   \]
   其中 \(\mathbf{x}\) 是输入向量，\(x_i\) 是向量中的第 \(i\) 个元素。

2. **Swish 函数**
   \[
   \text{Swish}(x, \beta) = x \cdot \sigma(\beta x)
   \]
   其中 \(\beta\) 是一个可学习的参数。

3. **Hardtanh 函数**
   \[
   \text{Hardtanh}(x) = \begin{cases} 
   1 & \text{if } x > 1 \\
   -1 & \text{if } x < -1 \\
   x & \text{otherwise}
   \end{cases}
   \]

